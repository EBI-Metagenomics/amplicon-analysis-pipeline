
import argparse
from collections import defaultdict
from multiprocessing import Pool
import pickle
import subprocess

import pandas as pd
from tqdm import tqdm

from utils import split_dir_into_sample_paths, get_read_count, build_cons_seq

# Script takes an input directory containing paired-end reads and outputs two dataframes (one for each strand)
# containing the total proportion of reads taken up by the 10 Most Common Prefixes (MCPs) for various MCP lengths
# on a per-sample basis i.e. rows: samples, columns: MCP lengths
# The MCPs are generated by the 'find_most_common_prefixes.sh' script
# This script is parallelised on a per-sample basis to speed up computation. Can specify cores to be used with -cpu

# Usage example:
# python lib/assess_mcp_proportions.py 
# -d /hps/nobackup/rdf/metagenomics/service-team/users/chrisata/asv_datasets/PRJNA714811/raw

def parse_args():
   
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dir", required=True, type=str, help="Input directory with both strands for every sample")
    parser.add_argument("-cpu", "--cpu", type=int, default=1, help="Number of CPUs")

    args = parser.parse_args()
    _DIR = args.dir
    _CPU = args.cpu

    return _DIR, _CPU

def fetch_mcp(file, prefix_len, beg='1', rev='0'):
    '''
    Extracts top 10 MCP from a read file. 
    Outputs resulting MCP counts as a dictionary:
        key -> MCP string
        val -> MCP count
    '''

    beg = str(beg)
    prefix_len = str(prefix_len)

    cmd = [
        'bash',
        '/hps/software/users/rdf/metagenomics/service-team/users/chrisata/asv_gen/bin/find_most_common_prefixes.sh',
        '-i',
        file,
        '-l',
        prefix_len,
        '-c',
        '10',
        '-b',
        beg,
        '-r',
        rev
    ]

    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()
    output = stdout.decode('ascii')
    
    output = output.strip()
    output = output.split('\n')

    mcp_count_dict = defaultdict(int)

    for line in output:
        line = line.strip()
        temp_lst = line.split(' ')
        if len(temp_lst) == 2:
            mcp_count_dict[temp_lst[1]] = int(temp_lst[0])

    return mcp_count_dict


def find_mcp_props_for_sample(sample):
    '''
    Extracts top 10 MCP for both strands of a sample for various MCP lengths.
    Outputs top 10 MCP proportions as dictionaries, one for each strand:
        key -> MCP length
        val -> MCP proportion
    '''

    res_dict_fwd = defaultdict(float)
    res_dict_rev = defaultdict(float)
    lengths_range = range(2, 25, 1)

    fileroot = sample.split('/')[-1]
    
    print(f'Processing {fileroot}')
    # TODO: need to automate fileroots 
    fwd = f"{sample}_1.fastq.gz"
    rev = f"{sample}_2.fastq.gz"

    subs_len = 4

    for beg in lengths_range:

        l = beg+subs_len
        l = str(l)

        fwd_mcp_count_dict = fetch_mcp(fwd, l, beg)
        rev_mcp_count_dict = fetch_mcp(rev, l, beg)
        fwd_mcp_cons_list = []
        rev_mcp_cons_list = []

        mcp_len = len(list(fwd_mcp_count_dict.keys())[0])
        for i in range(mcp_len):
            index_base_dict = defaultdict(int)
            for mcp in fwd_mcp_count_dict.keys():
                if len(mcp) <= subs_len:
                    continue
                base = mcp[i]
                index_base_dict[base] += fwd_mcp_count_dict[mcp]
            fwd_mcp_cons_list.append(index_base_dict)

        read_count = get_read_count(fwd)
        cons_seq, fwd_cons_conf = build_cons_seq(fwd_mcp_cons_list, read_count)

        mcp_len = len(list(rev_mcp_count_dict.keys())[0])

        for i in range(mcp_len):
            index_base_dict = defaultdict(int)
            for mcp in rev_mcp_count_dict.keys():
                if len(mcp) <= subs_len:
                    continue
                base = mcp[i]
                index_base_dict[base] += rev_mcp_count_dict[mcp]
            rev_mcp_cons_list.append(index_base_dict)

        read_count = get_read_count(rev)
        cons_seq, rev_cons_conf = build_cons_seq(rev_mcp_cons_list, read_count)

        
        fwd_mcp_sum = sum(fwd_mcp_count_dict.values())
        rev_mcp_sum = sum(rev_mcp_count_dict.values())

        fwd_prop = fwd_mcp_sum/read_count
        rev_prop = rev_mcp_sum/read_count

        # res_dict_fwd[beg] = fwd_prop
        # res_dict_rev[beg] = rev_prop

        res_dict_fwd[beg] = fwd_cons_conf
        res_dict_rev[beg] = rev_cons_conf

    return res_dict_fwd, res_dict_rev

def concat_parallel_out(out, sample_names):
    '''
    Concatenates per-sample output from find_mcp_props_for_sample() into one dataframe per strand.
    Dataframe shape:
        rows -> samples
        cols -> MCP lengths
    '''

    total_res_dict_fwd = defaultdict(list)
    total_res_dict_rev = defaultdict(list)

    for res in out:
        fwd = res[0]
        rev = res[1]

        [ total_res_dict_fwd[key].append(fwd[key]) for key in fwd.keys() ]
        [ total_res_dict_rev[key].append(rev[key]) for key in rev.keys() ]

    res_df_fwd = pd.DataFrame.from_dict(total_res_dict_fwd)
    res_df_rev = pd.DataFrame.from_dict(total_res_dict_rev)
    
    res_df_fwd.index = sample_names
    res_df_rev.index = sample_names

    return res_df_fwd, res_df_rev

def save_out(res_df_fwd, res_df_rev):
    '''Saves output dataframes into pickle files for later downstream analysis.'''

    print('Forward strand:')
    print(res_df_fwd)

    print('Reverse strand:')
    print(res_df_rev)
    
    subs_len = 4

    # fwd_out = f'{_DIR}/mcp_out/fwd_mcp_props_w{subs_len}.pkl'
    # rev_out = f'{_DIR}/mcp_out/rev_mcp_props_w{subs_len}.pkl'
    fwd_out = f'{_DIR}/mcp_out/fwd_mcp_cons_w{subs_len}_test2.pkl'
    rev_out = f'{_DIR}/mcp_out/rev_mcp_cons_w{subs_len}_test2.pkl'

    print(f'Saving forward strand results at {fwd_out}')
    with open(fwd_out, 'wb') as f:
        pickle.dump(res_df_fwd, f)

    print(f'Saving reverse strand results at {rev_out}')
    with open(rev_out, 'wb') as f:
        pickle.dump(res_df_rev, f)


def main():

    _DIR, _CPU = parse_args()

    sample_list = split_dir_into_sample_paths(_DIR)
    sample_names = [ sample.split('/')[-1] for sample in sample_list ]

    p = Pool(_CPU) # Prepare pool for parallelisation on per sample basis

    out = p.map(find_mcp_props_for_sample, sample_list)
    res_df_fwd, res_df_rev = concat_parallel_out(out, sample_names)
    save_out(res_df_fwd, res_df_rev)

    
if __name__ == "__main__":
    main()