#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2024 EMBL - European Bioinformatics Institute
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import argparse
from collections import defaultdict
import json
import pathlib
import logging

import pandas as pd
import pyfastx

logging.basicConfig(level=logging.DEBUG)

def parse_args():

    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--input", required=True, type=str, help="Input directory containing amplicon analysis pipeline results")
    parser.add_argument("-r", "--runs", required=True, type=str, help="CSV file containing successful analyses generated by the pipeline")
    parser.add_argument("-p", "--prefix", required=True, type=str, help="Prefix for the output file")

    args = parser.parse_args()

    INPUT = args.input
    RUNS = args.runs
    PREFIX = args.prefix

    return INPUT, RUNS, PREFIX

def get_read_count(read_path):

    fasta = pyfastx.Fasta(read_path, build_index=False)
    read_count = sum(1 for _ in fasta)

    return read_count

def add_markergene(root_path, run_acc, markergene_dict, markergene):

    if markergene != "ITS":

        bacterial_ssu = list(pathlib.Path(f"{root_path}/{run_acc}/sequence-categorisation").glob(f"*{markergene}*bacteria*"))
        archaeal_ssu = list(pathlib.Path(f"{root_path}/{run_acc}/sequence-categorisation").glob(f"*{markergene}*archaea*"))
        eukarya_ssu = list(pathlib.Path(f"{root_path}/{run_acc}/sequence-categorisation").glob(f"*{markergene}*eukarya*"))

        markergene_dict[f"{markergene} - bacteria"] = defaultdict()
        markergene_dict[f"{markergene} - archaea"] = defaultdict()
        markergene_dict[f"{markergene} - eukarya"] = defaultdict()

        markergene_dict = add_read_count_to_markergene(markergene_dict, bacterial_ssu, f"{markergene} - bacteria")
        markergene_dict = add_read_count_to_markergene(markergene_dict, archaeal_ssu, f"{markergene} - archaea")
        markergene_dict = add_read_count_to_markergene(markergene_dict, eukarya_ssu, f"{markergene} - eukarya")
    else:
        bacterial_ssu = list(pathlib.Path(f"{root_path}/{run_acc}/sequence-categorisation").glob("*ITS*"))
        markergene_dict["ITS"] = defaultdict()
        markergene_dict = add_read_count_to_markergene(markergene_dict, bacterial_ssu, "ITS")

    return markergene_dict

def add_read_count_to_markergene(marker_gene_dict, marker, label):

    if marker:
        read_count = get_read_count(str(marker[0]))
        marker_gene_dict[label]["read_count"] = read_count
    else:
        marker_gene_dict[label]["read_count"] = 0

    return marker_gene_dict

def main():
    
    INPUT, RUNS, PREFIX = parse_args()

    root_path = pathlib.Path(INPUT)

    if not root_path.exists():
        logging.error(f"Results path does not exist: {root_path}")
        exit(1)

    runs_df = pd.read_csv(RUNS, names=["run", "status"])

    # Marker gene study summary
    markergene_dict = defaultdict(dict)
    for i in range(0, len(runs_df)):
        run_acc = runs_df.loc[i, "run"]
        markergene_dict[run_acc]["marker_genes"] = defaultdict(dict)
        markergene_dict[run_acc]["marker_genes"] = add_markergene(root_path, run_acc, markergene_dict[run_acc]["marker_genes"], "SSU")
        markergene_dict[run_acc]["marker_genes"] = add_markergene(root_path, run_acc, markergene_dict[run_acc]["marker_genes"], "LSU")
        markergene_dict[run_acc]["marker_genes"] = add_markergene(root_path, run_acc, markergene_dict[run_acc]["marker_genes"], "ITS")

        total_read_counts = sum([ markergene["read_count"] for markergene in markergene_dict[run_acc]["marker_genes"].values() ])

        for markergene in markergene_dict[run_acc]["marker_genes"].keys():
            read_count = markergene_dict[run_acc]["marker_genes"][markergene]["read_count"]
            proportion = read_count / float(total_read_counts)
            
            if proportion >= 0.45:
                markergene_dict[run_acc]["marker_genes"][markergene]["majority_marker"] = True
            else:
                markergene_dict[run_acc]["marker_genes"][markergene]["majority_marker"] = False


    if markergene_dict:
        with open(f'{PREFIX}_markergene_study_summary.json', 'w') as fw:
            fw.write(json.dumps(markergene_dict, indent=4))
    else:
        logging.warning(f"Marker gene data empty for some reason. No summary file created.")

    # Amplified region study summary (only available if ASV results present)

    ampregion_dict = defaultdict(dict)
    for i in range(0, len(runs_df)):
        run_status = runs_df.loc[i, "status"]
        if run_status == "no_asvs":
            continue

        run_acc = runs_df.loc[i, "run"]
        ampregion_dict[run_acc]["amplified_regions"] = []

        amp_regions = list(pathlib.Path(f"{root_path}/{run_acc}/asv").glob("*S-V*/*.tsv"))

        for amp_region_path in amp_regions:
            amp_dict = defaultdict()
            amp_region = str(amp_region_path).split("/")[-2]
            marker_gene = amp_region.split("-")[0]
            amp_region = "-".join(amp_region.split("-")[1:])

            amp_region_df = pd.read_csv(amp_region_path, sep="\t")
            asv_count = len(amp_region_df)
            read_count = amp_region_df.loc[:, "count"].sum()

            amp_dict["marker_gene"] = marker_gene
            amp_dict["amplified_region"] = amp_region
            amp_dict["asv_count"] = int(asv_count) # casting needed for JSON serialising
            amp_dict["read_count"] = int(read_count) # casting needed for JSON serialising

            ampregion_dict[run_acc]["amplified_regions"].append(amp_dict)

    if ampregion_dict:
        with open(f'{PREFIX}_ampregion_study_summary.json', 'w') as fw:
            fw.write(json.dumps(ampregion_dict, indent=4))
    else:
        logging.warning(f"No amplified region data found. No summary file created.")


if __name__ == "__main__":
    main()